[
    {
        "author": "Hignette",
        "year": 2007,
        "title": {
            "text": "An Ontology-Driven Annotation of Data Tables",
            "link": ""
        },
        "conference-journal": "WISE",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Datatypes for each column are collected. The approach uses features of values defined in the ontology for numeric types to recognize the type of numeric columns, and the terms from the taxonomies of the symbolic types to recognize the type of symbolic columns. Column titles are also used to compute the score and choose the type of the column.",
            "type-annotation": "The approach links each cell in symbolic columns to its corresponding type in the ontology, using cosine similarity between weighted vectors. A set of candidate types is defined and a proportion is used to obtain the definitive cell type. Numeric cells' type is obtained by assigning a score to each possible numeric type, according to the cell content, and choosing the highest scoring.",
            "predicate-annotation": "The approach identifies relations by computing a similarity score for each predicate/relation in the ontology according to the table title and the column types.  ",
            "datatype-annotation": "The approach uses a set of manually defined rules to identify each cell's datatype as numeric, symbolic or unknown. A column is classified as symbolic if there are more cells classified as symbolic than numeric, numeric otherwise.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "web table",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": true
    },
    {
        "author": "Hignette",
        "year": 2009,
        "title": {
            "text": "Fuzzy Annotation of Web Data Tables Driven by a Domain Ontology",
            "link": ""
        },
        "conference-journal": "ESWC",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "similarity"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Datatypes for each column are collected; the approach computes a score of each symbolic type of the ontology for the column. This score is a combination of the score of the type for the column according to the column contents and the score of the type for the column according to the column title.",
            "type-annotation": "The approach links each cell in symbolic columns to its corresponding type in the ontology, using cosine similarity between weighted vectors. A set of candidate types is defined and a proportion is used to obtain the definitive cell type. Numeric cells' type is obtained by assigning a score to each possible numeric type, according to the cell content, and choosing the highest scoring.",
            "predicate-annotation": "The approach identifies relations by computing a similarity score for each predicate/relation in the ontology according to the table title and the column types. ",
            "datatype-annotation": "The approach uses a set of manually defined rules to identify each cell's datatype as numeric, symbolic or unknown. A column is classified as symbolic if there are more cells classified as symbolic than numeric, numeric otherwise.",
            "entity-linking": {
                "description": " ",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web table (XML)",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Tao",
        "year": 2009,
        "title": {
            "text": "Automatic hidden-web table interpretation, conceptualization, and semantic annotation",
            "link": ""
        },
        "conference-journal": "DKE",
        "name-of-approach": "TISP ++ (Table Interpretation with Sibling Pages)",
        "main-method": {
            "type": "unsup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Limaye",
        "year": 2010,
        "title": {
            "text": "Annotating and Searching Web Tables Using Entities, Types and Relationships",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The approach defines a vector of features composed by TFIDF cosine similarity between label in the header and label of concept",
            "predicate-annotation": "The approach defines a vector of features related to the presence and the frequency of the relation between the entities [C/R]",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "The approach define a vector of features composed by different similary measure, TFIDF cosine similarity, Jaccard between cell content and label of the entity [c]",
                "candidate-generation": "lookup, YAGO catalog",
                "entity-disambiguation": "features, similarity"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": "It mesures the compatibility of the annotations using a new set features: inverse document frequency (IDF), similarity/distance between E and T."
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web Table",
            "kg": {
                "triple-store": "Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Mulwad",
        "year": 2010,
        "title": {
            "text": "T2LD: Interpreting and Representing Tables as Linked Data⋆",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "T2LD",
        "main-method": {
            "type": "sup",
            "technique": "SVM"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "KBs are queried (e.g. Wikitology, DBpedia) to identify the best classes for each column",
            "predicate-annotation": "The approach generates a set of candidate relations from the relations that exist between the strings in each row of the columns. ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "An additional query to Wikitology is submitted to retrieve the proper entity, using the class labels from the column as additional evidence. The top N entities are returned; a feature vector is created for each cell and the vectors are ranked using a SVM-rank classifier. Then, the highest scoring feature vector is enriched and a second SVM classifier decides whether to link the table cell to the top ranked entity.",
                "candidate-generation": "lookup, wikitology",
                "entity-disambiguation": "features, ML"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "Wikitology",
                "index": ""
            }
        },
        "output-format": "N3",
        "checked-by-author": false
    },
    {
        "author": "Syed",
        "year": 2010,
        "title": {
            "text": "Exploting a Web of Semantic Data for Interpreting Tables",
            "link": ""
        },
        "conference-journal": "WSC",
        "name-of-approach": "Wikitology",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "The column with the 'title' label is assumed as subject",
            "column-analysis": "",
            "type-annotation": "The class for the column is obtained from the entities class",
            "predicate-annotation": "A set of relations between a pair of entities is returned by Wikitology. The relation that appears most often in the column is selected.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Each instance in the table is mapped to the title and redirect field in Wikitology using an IR index based on Lucene (= the title of Wikipedia articles and redirects). The table header is mapped to types from YAGO, DBpedia, Freebase, WordNet. Entity mention and column header mapped to the firstSentence field (first sentence in Wikipedia). In this way, WIkitology returns the top N possible Wikipedia entities for each string",
                "candidate-generation": "lookup, Wikitology",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Wikipedia tables",
            "kg": {
                "triple-store": "Wikitology",
                "index": "Lucene for concepts"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Mulwad",
        "year": 2011,
        "title": {
            "text": "Automatically Generating Government Linked Data from Tables",
            "link": ""
        },
        "conference-journal": "AAAI",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network + PGM"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The algorithm determines the class for a table column based on the class of the individual strings in the column [C]",
            "predicate-annotation": "The relation between entities that gets majority vote is chosen as the relation between the columns [R]",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Using the predicted class labels for column, the algorithm collect entities. For every entities they create a feature vector, which are ranked using a SVM Rank classifier [C]",
                "candidate-generation": "lookup, wikitology",
                "entity-disambiguation": "features, ML"
            },
            "nil-annotation": "These feature vectors are ranked using an SVM-Rank classifier. If the evidence is not strong enough, it suggests that the table cell represents a new entity not present in the \\ac{kg}, which is useful for discovering new entities in the table."
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "Human in the loop"
        },
        "validation": "Custom GS, 15 tables",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV, XML",
            "kg": {
                "triple-store": "DBpedia,Freebase,WordNet,Yago",
                "index": ""
            }
        },
        "output-format": "N3",
        "checked-by-author": false
    },
    {
        "author": "Venetis",
        "year": 2011,
        "title": {
            "text": "Recovering Semantics of Tables on the Web",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "Using the SVM method we were able to identify the subject column in 94% of the tables.",
            "column-analysis": " ",
            "type-annotation": "The column label is retrieved based on the isA database (instance, class). The boundaries of potential class label in the text are obtained by using the TnT POS tagger; To find the best class for a column, they maximize the probbaility of the values given the class label for the column.",
            "predicate-annotation": "Given two columns, they look for corresponding pairs of values in the columns. If the relation R is extracted for many rows, then R is a likely relation represented by the column. They use TextRunner to extract triples to build a relation database. ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "To create a gold standard for labeling tables, a random sample of tables was taken. Tables without any class or relation labels assigned by the Majority algorithm with a permissive labeling threshold of 10% (R10) were removed. Tables with incorrectly identified subject columns or lacking meaningful concepts were manually eliminated. The remaining 168 tables were presented to human annotators along with the labels generated by the R10 algorithm"
        },
        "validation": "Custom GS",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Goel",
        "year": 2012,
        "title": {
            "text": "Exploiting Structure within Data for Accurate Labeling using Conditional Random Fields",
            "link": ""
        },
        "conference-journal": "ICAI",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "CRF"
        },
        "domain": {
            "domain": "dependent",
            "type": "Weather forecast, flight status and geocoding"
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Strings are split at whitespaces and the resulting tokens are split so that the resultant parts are purely alphabetic, numeric or a single simbol character.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The approach uses a contitional random field CRM to learn the labeling function, usign a graph structure of the features of every token. To predict the most likely label assignment to the field nodes they use Viterbi Algorithm.",
            "predicate-annotation": "The accuracy also increases when we exploit the relationship between the field labels and the token labels.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Knoblock",
        "year": 2012,
        "title": {
            "text": "Semi-automatically Mapping Structured Sources into the Semantic Web",
            "link": ""
        },
        "conference-journal": "ESWC",
        "name-of-approach": "Karma",
        "main-method": {
            "type": "sup",
            "technique": "CRF"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "The approach uses a contitional random field CRF to learn the labeling function. It uses a feature vector that characterizes the syntactic structure of the column name and the value.",
            "predicate-annotation": "Using the concept annotation, it creates different nodes. For each pair of nodes, it extracts the relation between them from the KG. To identify the right predicate,they use a Stiner Tree Algorithm for computing the minimal tree.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "The user can select the correct annotation using the UI. The system  considers the changes oparated by the users."
        },
        "validation": "",
        "code-availability": "https://github.com/usc-isi-i2/Web-Karma",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Data sources Ontologies Semantic Types (training dataset)",
            "kg": {
                "triple-store": "Personal ontologies",
                "index": ""
            }
        },
        "output-format": "Source model (used to generate RDF)",
        "checked-by-author": false
    },
    {
        "author": "Pimplikar",
        "year": 2012,
        "title": {
            "text": "Answering Table Queries on the Web using Column Keywords",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "probabilistic graphical model"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "A unified approach based on graphical models to jointly label columns is proposed. Each column has to be labeled with one of the keywords in the query; a similarity score is computed between the keywords and the header tokens/the context of the table. The TF-IDF weighted cosine similarity is computed between the query term and the header. The sum of the soft-maxed match reliability of each term in the query weighted by the TF-IDF score of the term is the similarity between the term and the rest of the table.",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Wang",
        "year": 2012,
        "title": {
            "text": "Understanding Tables on the Web",
            "link": ""
        },
        "conference-journal": "ER",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "pattern matching"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "For each column, a concept is derived based on the assumption that entities in a column should belong to the same concept. A candidate schema is generated according to the header of each column; for each schema in the candidate schema list, they enumerate every column and compute a confidence score (they use Probase)",
            "type-annotation": "For a given concept, Probase is used to find its entities and its attributes and viceversa",
            "predicate-annotation": "pattern includes two main factors: the relation between entity and concept, and the relation between attribute and concept.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "It uses the Probase taxonomy which is a probabilistic taxonomy. Column type is used to filter candidates",
                "candidate-generation": "pattern matching",
                "entity-disambiguation": "features"
            },
            "nil-annotation": "we can expand Probase by incorporating this new evidence in the same way as we encounter a new Hearst’s pattern."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Custom, 200 wikipedia tables",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "HTML tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Buche",
        "year": 2013,
        "title": {
            "text": "Fuzzy Web Data Tables Integration Guided by an Ontological and Terminological Resource",
            "link": ""
        },
        "conference-journal": "IEEE",
        "name-of-approach": "ONDINE",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": "microbial risk, chemical risk, aeronautics"
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Distinction between symbolic and numerical columns",
            "type-annotation": "A score for each column is computed leveraging: 1. The column title; 2. The column content (computed as term similarity between the terms in the column and the terms in the OTR). wrt the concepts that appear in the signature of the relations belonging to the OTR.",
            "predicate-annotation": "Each relation in the OTR is scored, using a proportion of simple concepts in the signature of the relation which are represented by columns in the table.",
            "datatype-annotation": "Symbolic and numerical columns are distinguished, using knowledge in an Ontological and Terminological Resource (OTR) i.e., unit concepts. ",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Custom, 90 tables",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Domain specific web tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "XML documents representing data tables; RDF annotations",
        "checked-by-author": false
    },
    {
        "author": "Cruz",
        "year": 2013,
        "title": {
            "text": "GIVA: A Semantic Framework for Geospatial and Temporal Data Integration, Visualization, and Analytics",
            "link": ""
        },
        "conference-journal": "SIGSPATIAL",
        "name-of-approach": "Giva",
        "main-method": {
            "type": "sup",
            "technique": "AgreementMaker"
        },
        "domain": {
            "domain": "dependent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "They translate the data into a common spatial data format.",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "They use string matching on the column header and perform random sampling on the values to find pattern similarities (to identify the column that contains spatial data).",
            "type-annotation": "",
            "predicate-annotation": "Geospatial classification schemes can be modeles using a part-of or is-a relationship",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "GML, KML, Shapefile, MapInfo TAB, HTML table",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Deng",
        "year": 2013,
        "title": {
            "text": "Scalable Column Concept Determination for Web Tables Using Large Knowledge Bases",
            "link": ""
        },
        "conference-journal": "VLDB",
        "name-of-approach": "lookup",
        "main-method": {
            "type": "unsup",
            "technique": ""
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "A similarity score between the column and the identified types is computed. They extend the MapReduce-based similarity joins to support scalable column concept determination. 1. For each column, the list of types that share at least one entity/cell  value with the column; 2. Compute top-k types of every column; 3. Compute the overlap similarity. Both exact-matching and fuzzy-matching similarity functions are employed.",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web",
            "kg": {
                "triple-store": "DBpedia,Freebase,Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ermilov",
        "year": 2013,
        "title": {
            "text": "User-driven Semantic Mapping of Tabular Data",
            "link": ""
        },
        "conference-journal": "I-SEMANTICS",
        "name-of-approach": "",
        "main-method": {
            "type": "",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "The RDF mapping obtained from Sparqlify-CSV a page is created for each resource on the mappings wiki, which uses the Semantic MediaWiki extension to allow for semantic annotations over the content of such pages. "
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "CSV, TSV, XLS, XLSX",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Mulwad",
        "year": 2013,
        "title": {
            "text": "Semantic Message Passing for Generating Linked Data from Tables",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Acronyms are expanded and stylized literal values (e.g. phone numbers) are recognized",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "The query and rank module identifies a list of possible candidate assignments using data from DBpedia, YAGO and Wikitology. The column potential classes are the union of the classes of its cells (from DBpedia and YAGO).",
            "type-annotation": "The initial entities assigned to a column’s cell values perform a majority voting over the YAGO and DBpedia class set to pick the top YAGO and DBpedia class. Each entity votes and increments the score of a class; the YAGO and DBpedia sets are then ordered and the top classes are considered. Each class is assigned a confidence score; if the score is below a certain threshold, the column header is assigned to NO-ANNOTATION",
            "predicate-annotation": "The approach uses the links between pairs of entities to generate candidate relations. For a pair of cell values in the same row between two columns, the candidate entity sets for both cells are obtained. For each possible pairing, YAGO and DBpedia are queried to obtain relations in either direction. Also, relations between columns are considered.",
            "datatype-annotation": "We use a regular expression to distinguish string mentions, which probably refer to entities, and literal constants such as numbers and measurements, which probably do not. If the cell value is a literal constant,candidate entities are not generated and the cell is mapped to no-annotation.",
            "entity-linking": {
                "description": "A set of candidate entities for each cell is generated using Wikitology (which uses DBpedia, Wikipedia and YAGO). An entity ranker re-ranks a cell's candidate entities using an approach mutuated from literature. ",
                "candidate-generation": "lookup, wikitology",
                "entity-disambiguation": "features, ML"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Web Manual, Web Relation, Wiki Manual, Wiki Links",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web and Wikipedia HTML tables",
            "kg": {
                "triple-store": "DBpedia,Yago,Wikitology",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Munoz",
        "year": 2013,
        "title": {
            "text": "Triplifying Wikipedia’s Tables",
            "link": ""
        },
        "conference-journal": "LD4IE",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "exact match"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Each page Wikipedia containing HTML tables is cleaned and canonicalized before extracting tables fixing syntax mistakes using CyberNeko",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": "DBpedia is queried for relationships that exist between entities in cells of the same row. All pairs are queried. An additional query is performed to find the relation between entities in a row and the subject of the page the table belongs to. They look for relations that hold in either direction.",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "Each link in a cell is mapped to a DBpedia entity URI by following redirects and replacing namespace http://en.wikipedia.org/wiki/ to http://DBpedia.org/resource/",
                "candidate-generation": "no candidate since the purpose is to extract rdf triples",
                "entity-disambiguation": "redirects"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "They extract 250 triples from the set and manually annotate them to produce a GS to test the approach on.",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Wikipedia tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Quercini",
        "year": 2013,
        "title": {
            "text": "Entity Discovery and Annotation in Tables",
            "link": ""
        },
        "conference-journal": "EDBT",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "lookup"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "Spatial data are broken down into their components via the Google Geocoding API. To train the multiclass classifier they convert snippets to lowercase and tokenize. Then they remove stopwords and stem the reimainders with the Porter algorithm. Each token is then associated with its normalized frequency",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "Cells are scanned so as to eliminate those that are not likely to contain the names of entities of a given type. They look at the syntactic properties of the contents of each cell. They use regex to identify patterns of e.g. phone numbers, URLs, emails, or leverage GFT types to rule out the cells containing a specific type of information that is not necessary to identify a pre-determined type (e.g. cells that match the email pattern are excluded when identifying cells belonging to the restaurant type). ",
            "type-annotation": "They perform type (concepts in ontology) annotation at cell-level, so that each column can contain more than one type. Once the table has been annotated, based on the assumption that data types in a column are homogenous in well-formed tables, incorrect annotations are discarded",
            "predicate-annotation": " ",
            "datatype-annotation": "regex",
            "entity-linking": {
                "description": "The approach first identifies row containing information on a specific type (obtained from an ontology) then determines the cells that contain the names of those entities. The multiclass classifier is trained on a training dataset (of snippets of positive examples of a pre-defined type) by: 1. Creating a set of entities from DBpedia belonging to categories that are manually identified as useful; 2. Collecting snippets from Bing; 3. 75% of the snippet corpus is training set, 25% is the test set.  After the text classifier (either SVM or Naive Bayes in experiments) has classified cells, spurious annotations are removed. ",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": "Information about unknown entities is found on the Web so as to annotate them with the correct type. They use the content of the cell to query the Web and use the obtained results (which are links to Web pages+snippet) to determine whether the cell contains the name of an entity of a certain type. The result of a query is a snippet and they use a text classifier to determine whether the snippet describes an entity of a certain type. "
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Google Fusion Tables",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zhang",
        "year": 2013,
        "title": {
            "text": "InfoGather+: Semantic Matching and Annotation of Numeric and Time-Varying Attributes in Web Tables",
            "link": ""
        },
        "conference-journal": "SIGMOD",
        "name-of-approach": "InfoGather+",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": "We assume that the set of conversion rules are known up- front. They are specified by the system administrator (or domain experts) using a simple, rule specification language. Each rule has 3 components: a left-hand side (LHS), a con- version factor (θ) and a right-hand side (RHS). The LHS and RHS are strings describing units and scales. For exam- ple, for the rule Euro = 1.3 × USD, the LHS is Euro, θ is 1.3 and the RHS is USD. Figure 3(a) shows more examples of rules."
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "They infer label information from semantically matching columns of other Web tables. Given a set of pre-defined rules and: Case 1: a set of unit/scale descriptors (eg USD, EUR etc), they annotate the column with the value if it (or one of its synonyms) either appears in the header or column values. Case 2: a set of years strings, they annotate the column with the value if it occurs in the header or in the context of the table.",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": ""
        },
        "validation": "Experiments conducted on three real-life datasets of web tables,extracted from a recent snapshot of Microsoft Bing search engine",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "EAB (entity-attribute binary relationships) HTML Web tables ",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Zwicklbauer",
        "year": 2013,
        "title": {
            "text": "Towards Disambiguating Web Tables",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "majority voting",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Subset of Limaye",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "DBpedia",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Sekhavat",
        "year": 2014,
        "title": {
            "text": "Knowledge Base Augmentation using Tabular Data",
            "link": ""
        },
        "conference-journal": "LDOW",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "probabilistic model"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": "patty patterns",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "ClueWeb09 dataset",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "",
            "kg": {
                "triple-store": "",
                "index": "Yago"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Taheriyan",
        "year": 2014,
        "title": {
            "text": "A Scalable Approach to Learn Semantic Models of Structured Sources",
            "link": ""
        },
        "conference-journal": "IEEE",
        "name-of-approach": "",
        "main-method": {
            "type": "unsup",
            "technique": "features"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "For each attribute in the data source, a set of candidate sematnic types (with confidence level) is collected and ranked. To learn semantic types, they use a supervised machine learning technique based on Conditional Random Fields (CRF) with features extracted from the attribute names and sample data from the new source. Type = ontology class or data-property + domain-class pair",
            "predicate-annotation": "To learn relationships between types, the authors leverage previously built semantic models belonging to the same domain (e.g. museums) assuming that similar sources will share similar semantic models. To annotate relationships, they use a directed weighted graph 𝐺 built on top of the known semantic models and expanded using the semantic types 𝑇 and the domain ontology 𝑂; properties are weighted links between nodes. The algorithm used to construct 𝐺: 1. Adds known semantic models: each model is added if it is not a subgraph of already added components; 2. Adds semantic types, checking for each type if the graph contains a match for it; 3. Adds paths from the ontology, using the subclass hierarchy in the ontology",
            "datatype-annotation": "They assign a class to attributes whose values are URIs and assign a domain/data property pair to attributes containing literal values (see Type/concept annotation column)",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": "We applied our approach on this dataset to find the candidate semantic models for each source and then compared the best suggested models with models created manually by domain experts"
        },
        "validation": "We evaluated our approach on a dataset of 29 museum data sources",
        "code-availability": "",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Multiple sources modeled using the EDM, AAC, SKOS, Dublin Core Metadata Terms, FOAF, ORE, and ElementsGr2 ontologies",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Bhagavatula",
        "year": 2015,
        "title": {
            "text": "TabEL: Entity Linking in Web Tables",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "TabEL",
        "main-method": {
            "type": "sup",
            "technique": "Markov Network"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "TabEl performs EL in three steps: 1. Mention identification; 2. Entity candidate generation; 3. Disambiguation. It also relies on a prior estimate that a given string refers to a particular entity, using an estimate distribution from hyperlinks on the Web and in Wikipedia.  The number of in-links to an entity in Wikipedia is an indicator of its prominence",
                "candidate-generation": "lookup, YAGO",
                "entity-disambiguation": "features, probabilistic"
            },
            "nil-annotation": "TabEL demonstrates strong performance in identifying and disambiguating unlinked mentions in Wikipedia tables. Unlike previous experiments that removed all existing links, TabEL for this task retains the existing links."
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "TabEL",
        "code-availability": "http://websail-fe.cs.northwestern.edu/TabEL/ ",
        "license": "CCA 4.0",
        "inputs": {
            "type-of-table": "Web table (XML)",
            "kg": {
                "triple-store": "Yago",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ramnandan",
        "year": 2015,
        "title": {
            "text": "Assigning Semantic Labels to Data Sources",
            "link": ""
        },
        "conference-journal": "ESWC",
        "name-of-approach": "SemanticTyper",
        "main-method": {
            "type": "sup",
            "technique": "CRF"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "For textual data, the authors use the TF-IDFbased approach and for numeric data, they use the Kolmogorov-Smirnov (KS) statistical hypothesis test. In order to resolve column heterogeneity the authors adopted the rule that in the training data, if for a semantic label the fraction of pure numeric data values is below 60%, it is trained as textual data (and hence indexed as document). If the fraction of numeric values is above 80%, it is trained as purely numeric data (its distribution is extracted to be used in KS test) after discarding textual data values. In the other case (if the fraction is between 60% and 80%), the data is trained as both textual and numeric data (it is both indexed as a document and its ",
            "type-annotation": "TF-IDF",
            "predicate-annotation": " ",
            "datatype-annotation": "Welch’s t-test",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Semi-automated",
            "description": ""
        },
        "validation": "we used data from the museum domain consisting of 29 data sources in diverse formats from various art museums in the U.S. Semantic labels",
        "code-availability": "https://github.com/usc-isi-i2/eswc-2015-semantic-typing",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "",
                "index": "training data with Lucene, not KG data"
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ritze",
        "year": 2015,
        "title": {
            "text": "Matching HTML Tables to DBpedia",
            "link": ""
        },
        "conference-journal": "WIMS",
        "name-of-approach": "T2K Match",
        "main-method": {
            "type": "unsup",
            "technique": "feature"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "remove HTML artifacts, special characters and additional whitespaces. Further, value lists are split into individual values, all values are lower-cased and normalized. For the normalization, we use a set of handcrafted transformation rules to resolve abbreviations, e.g. “co.” is transformed into “company”. Moreover, the header is detected",
                "spell-checker": "",
                "units-of-measurements": "Further, we normalize units of measurements using around 200 manually generated conversion rules, e.g. 8mi2 is converted to 20.72 million m2."
            },
            "subject-detection": "",
            "column-analysis": "We further perform a data type detection for each attribute which is important for choosing the similarity measure. The data type is detected using about 100 manually defined regular expressions. They are able to detect string, numeric value, timestamp and coordinate. The final data type decision for an attribute is based on majority vote.",
            "type-annotation": "we determine the distribution of DBpedia classes of the best candidate for each entity and choose the most frequent classes as candidates for schema matching.",
            "predicate-annotation": "Each value of the top candidates votes for a correspondence between the attribute and its property. This vote is weighted by the value-based similarity of the two values and the similarity value from the candidate selection step. Votes from all values are summed up and the attribute property pair with the highest value is chosen. It follows the intuition that a similar attribute property pair has many similar values on similar entities/candidates. Our computation is purely duplicate-based and does not perform any label matching on the headers since headers in HTML tables do not often have meaningful names. The matching of attributes with properties is further aggregated by summing up all the scores of all property correspondences per class to refine the class ranking. At this point we choose the class with the highest score as final correspondence.",
            "datatype-annotation": "We further perform a data type detection for each attribute which is important for choosing the similarity measure. The data type is detected using about 100 manually defined regular expressions. They are able to detect string, numeric value, timestamp and coordinate. The final data type decision for an attribute is based on majority vote.",
            "entity-linking": {
                "description": "First, we search for the entity label in DBpedia. The found candidates are ranked according to a similarity function and the top k candidates are kept. Then, we determine the distribution of DBpedia classes of the best candidate for each entity and choose the most frequent classes as candidates for schema matching. In turn, this is used to refine the candidate resources: All candidates not belonging to a chosen class are removed, and for each entity we perform another search with the selected classes as additional constraint. We compute similarities between the values of HTML table entities and candidate resources by applying two blocking strategies: (1) the values of each entity are only compared to the values of its candidates and (2) only values with the same data type are compared. In case of multi-values, we calculate the similarity of all combinations and choose the maximum.",
                "candidate-generation": "lookup, DBP LS",
                "entity-disambiguation": "features, contextual inf"
            },
            "nil-annotation": "The article explicitly mentions that with the T2K approach one could fill the missing values to DBpedia and vice-versa (Web Tables). Howver, they do no provide any evidence on how they store or use new entities / properties / values in DBpedia. "
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2D",
        "code-availability": "https://github.com/olehmberg/T2KMatch",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "Web Table (HTML)",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Ermilov",
        "year": 2016,
        "title": {
            "text": "TAIPAN: Automatic Property Mapping for Tabular Data",
            "link": ""
        },
        "conference-journal": "EKAW",
        "name-of-approach": "TAIPAN",
        "main-method": {
            "type": "unsup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "After characterizing columns by means of their support and connectivity scores, we can use binary classifiers to classify columns of a table as being either subject columns or not. Best classifiers SVM, Decision Tree",
            "column-analysis": " ",
            "type-annotation": "Approach takes into account the relation between the S-column and the other NE columns. It uses a probabilistic model based on the most frequent types in the \\ac{kg} and majority voting to determine the column type annotation.",
            "predicate-annotation": "relation probability: number of matching pairs / number of table rows",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "Improved T2D",
        "code-availability": "https://github.com/dice-group/TAIPAN",
        "license": "GPL 3.0",
        "inputs": {
            "type-of-table": "Not specified, they mention only tables. DBpedia Table Dataset (DBD)",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Neumaier",
        "year": 2016,
        "title": {
            "text": "Multi-level semantic labelling of numerical values",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "sup",
            "technique": "Hierarchical clustering (BKG)"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "Only numerical values.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "by splitting the DBpedia data into a test and training dataset.",
        "code-availability": "https://github.com/sebneu/number_labelling",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV",
            "kg": {
                "triple-store": "DBpedia",
                "index": ""
            }
        },
        "output-format": "",
        "checked-by-author": false
    },
    {
        "author": "Pham",
        "year": 2016,
        "title": {
            "text": "Semantic labeling: A domain-independent approach",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "DSL(Domain-independent SemanticLabeler)",
        "main-method": {
            "type": "sup",
            "technique": "Logistic Regression"
        },
        "domain": {
            "domain": "independent",
            "type": "Musum, city, soccer, weather"
        },
        "tasks": {
            "cta": true,
            "cpa": false,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "In our approach, we compute three different value similarity metrics: Jaccard similarity and TF-IDF cosine similarity for textual data, as well as a modified version of Jaccard similarity for numeric values. For numeric data, there are semantic types that we are unable to distinguish by using value similarity because they have the same range of values. Therefore, we analyze the distribution of numeric values contained in the attributes using statistical hypothesis testing as one of the similarity metrics. We use KolmogorovSmirnov test (KS test)[6] as our statistical hypothesis test based on evaluation of different statistical tests in Ramnandan et al.’s research ",
            "type-annotation": "",
            "predicate-annotation": " ",
            "datatype-annotation": "In our approach, we compute three different value similarity metrics: Jaccard similarity and TF-IDF cosine similarity for textual data, as well as a modified version of Jaccard similarity for numeric values.",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "T2D",
        "code-availability": "https://github.com/minhptx/iswc-2016-semantic-labeling",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "The input of our system is an unlabeled attribute and a set of labeled attributes as domain data.",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "The output is a set of top-k semantic types corresponding to the unlabeled attribute.",
        "checked-by-author": false
    },
    {
        "author": "Taheriyan",
        "year": 2016,
        "title": {
            "text": "Learning the semantics of structured data sources",
            "link": ""
        },
        "conference-journal": "JOWS",
        "name-of-approach": "Karma",
        "main-method": {
            "type": "sup",
            "technique": "Cosine similarity + statistical hypothesis"
        },
        "domain": {
            "domain": "dependent",
            "type": "museum"
        },
        "tasks": {
            "cta": true,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": "syntactic information about data sources such as attribute names or attribute types (string, int, date, ...) may give the system some hints to discover semantic types. We employ the technique proposed by Ramnandan2015 to learn semantic types of source attributes. Their approach focuses on learning the semantic types from the data rather than the attribute names. It learns a semantic labeling function from a set of sources that have been manually labeled. ",
            "type-annotation": "For each class node v in sm(si), we search the graph to see if G includes a class node with the same label. It is possible that sm(si) contains multiple class nodes with the same label, for instance, a model including the link isFriendOf from one Person to another Person. In this case, we make sure that G also has at least the same number of class nodes with that label. Once we added the required class nodes to the graph, we map the class nodes in the model to the class nodes in the graph. If G has multiple class nodes with the same label, we select the one that is tagged by larger number of known semantic models. We add anentry to H with v as the key and the mapped node(v′) as the value.",
            "predicate-annotation": "The goal is to connect class nodes of G using the direct paths or the. paths inferred through the subclass hierarchy in O. Once we apply this labeling method, it generates a set of candidate semantic types for each source attribute, each with a confidence value. Our algorithm then selects the top k semantic types for each attribute as an input to the next step of the process. 11 ,...,tp1k Thus, the output of the labeling step for s(a1,a2,...,am) is T = {(tp11 1k ),...,(tpm1 m1 ,...,tpmk mk )}, where in tpij ij , tij is the jth semantic type learned for the attribute ai and pij is the associated confidence value which is a decimal value between 0 and 1.",
            "datatype-annotation": "If the data values associated with a source attribute ai are textual data, the labeling algorithm uses the cosine similarity between TF/IDF vectors of the labeled documents and the input document to predict candidate semantic types. For attributes with numeric data, the algorithm uses statistical hypothesis testing [25] to analyze the distribution of numericvalues.",
            "entity-linking": {
                "description": "Therefore, we adopt a different strategy; if there is more than one node in the graph matching a node in the semantic model, we select the one having moretags.",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": "Not only entities. When adding a new node or link, we tag it with a unique identifier (e.g., si, name of the source) indicating that the node/link exist in sm(si)."
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": "Users then interact with the system to adjust the automatically generated model. During this process, users can transform the data as needed to normalize data expressed in different formats and to restructure it."
        },
        "validation": "Museum data, eventhough they say that the use some tables of museum data from the gold standard it does not mention which GS they use. The link to the datasets https://github.com/taheriyan/jws-knowledge-graphs-2015",
        "code-availability": "https://github.com/usc-isi-i2/Web-Karma",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "XML files and JSON files. They can also import the domain ontologies they want to use for modeling the data. The system then automatically suggests a semantic model for the loaded source.",
            "kg": {
                "triple-store": "CIDOC-CRM,EDM",
                "index": ""
            }
        },
        "output-format": "RDF",
        "checked-by-author": false
    },
    {
        "author": "Taheriyan",
        "year": 2016,
        "title": {
            "text": "Leveraging Linked Data to Discover Semantic Relations within Data Sources",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "Karma",
        "main-method": {
            "type": "sup",
            "technique": "pattern"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": false,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "",
            "column-analysis": " ",
            "type-annotation": "",
            "predicate-annotation": "Auhtors exploit the predicates used by LOD datasets, by extracting all predicates that connect two given classes in the LOD. BANKS algorithm computes the top k minimum cost trees that span a subset of the nodes in a graph (the nodes that the semantic types are mapped to).",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "",
                "candidate-generation": "",
                "entity-disambiguation": ""
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "Fully-automated",
            "description": "GUI usage"
        },
        "validation": "",
        "code-availability": "https://github.com/usc-isi-i2/Web-Karma",
        "license": "Apache 2.0",
        "inputs": {
            "type-of-table": "CSV, XML and JSON",
            "kg": {
                "triple-store": "CIDOC-CRM",
                "index": ""
            }
        },
        "output-format": "a semantic model expressing how the assigned labels are connected",
        "checked-by-author": false
    },
    {
        "author": "Efthymiou",
        "year": 2017,
        "title": {
            "text": "Matching Web Tables with Knowledge Base Entities: From Entity Lookups to Entity Embeddings",
            "link": ""
        },
        "conference-journal": "ISWC",
        "name-of-approach": "",
        "main-method": {
            "type": "hybrid",
            "technique": "Hybrid"
        },
        "domain": {
            "domain": "independent",
            "type": ""
        },
        "tasks": {
            "cta": false,
            "cpa": true,
            "cea": true,
            "cnea": false
        },
        "steps": {
            "data-preparation": {
                "description": "stopword removal",
                "spell-checker": "",
                "units-of-measurements": ""
            },
            "subject-detection": "the label column (that I think refers to the subject column) is defined as the leftmost column with the maximum number of distinct (non-numeric) values",
            "column-analysis": "column sampling",
            "type-annotation": "",
            "predicate-annotation": "associate each column with an object property having as domain the ontology class to which the subject column entity belongs",
            "datatype-annotation": "",
            "entity-linking": {
                "description": "(a) a lookup-based method which relies on the minimal entity context provided in Web tables to discover correspondences to the KB, (b) a semantic embeddings method that exploits a vectorial representation of the rich entity context in a KB to identify the most relevant subset of entities in the Web table, and (c) an ontology matching method, which exploits schematic and instance information of entities available both in a KB and a Web table.",
                "candidate-generation": "lookup, custom index",
                "entity-disambiguation": "entity embedding, contextual inf"
            },
            "nil-annotation": ""
        },
        "user-revision": {
            "type": "none",
            "description": ""
        },
        "validation": "TD2, Limaye, Wikipedia (a GS constructed by the authors created by extracting the hyperlinks of existing Wikipedia tables to Wikipedia pages, which we have replaced with annotations to the corresponding entities from the October 2015 version of DBpedia.) Since the header rows in Wikipedia tables are not linked to properties, our gold standard does not contain schema-level mappings.",
        "code-availability": "http://www.cs.toronto.edu/~oktie/webtables/",
        "license": "NotSpecified",
        "inputs": {
            "type-of-table": "Web tables",
            "kg": {
                "triple-store": "",
                "index": ""
            }
        },
        "output-format": "JSON",
        "checked-by-author": false
    }
]